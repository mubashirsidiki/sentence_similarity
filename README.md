# Sentence Similarity Analysis Project

This project demonstrates two approaches to measuring sentence similarity using machine learning: a **Transformer-based approach** and a **Siamese Neural Network approach**.

## ðŸŽ¬ Original Use Case: Rotten Tomatoes Movie Review Analysis

This project explores sentence similarity through movie reviews, comparing audience vs. critic consensus on movies from Rotten Tomatoes.

**Research Question:** "Do critics and audiences really have such different opinions about movies, or are we overstating their differences?"

I gathered data from Rotten Tomatoes for top box office movies of 2024, comparing audience consensus vs. critic consensus using NLP techniques.

## ðŸ“‹ Table of Contents

- [Approach 1: Transformer Neural Network](#approach-1-transformer-neural-network)
- [Approach 2: Siamese Neural Network](#approach-2-siamese-neural-network)
- [Datasets](#datasets)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)

## ðŸš€ Approach 1: Transformer Neural Network

### What is it?
Uses the **BGE-M3** transformer model to convert sentences into numerical vectors that capture semantic meaning.

### How it works:
1. **Model**: BGE-M3 (ranked #2 on MTEB leaderboard)
2. **Embedding**: Converts sentences to 1024-dimensional vectors
3. **Similarity**: Uses cosine similarity to measure vector similarity
4. **Classification**: "Strong" (â‰¥0.7), "Moderate" (â‰¥0.5), "Weak" (<0.5)

### Example:
```python
similarity = compute_similarity("I had a bad day", "Everything was terrible today")
# Returns: 0.797 (Strong similarity)
```

## ðŸ§  Approach 2: Siamese Neural Network

### What is it?
A custom neural network that learns to compare sentence pairs using bidirectional LSTM layers.

### Architecture:
- **Embedding**: 50-dimensional Word2Vec
- **LSTM**: 50 units (bidirectional)
- **Dense**: 50 units
- **Dropout**: 17% (LSTM), 25% (Dense)

### Limitations:
- Small dataset (193 pairs, needs 1,000+)
- Poor performance (~47% validation accuracy)
- Early stopping due to overfitting

## ðŸ“Š Datasets

### 1. Movie Reviews Dataset (`audience_vs_critic_pronoun.csv`)
- **Source**: Scraped from Rotten Tomatoes (top box office movies 2024)
- **Content**: Movie names, audience consensus, critic consensus
- **Size**: 17 movie entries

### 2. Sentence Pairs Dataset (`sentences.csv`)
- **Source**: Generated by ChatGPT
- **Content**: Sentence pairs with binary similarity labels
- **Size**: 193 sentence pairs

## ðŸ› ï¸ Installation

```bash
# Core dependencies
pip install sentence-transformers matplotlib python-dotenv openai seaborn pandas numpy

# For Siamese network
pip install keras pandas gensim tensorflow tensorboard
```

## ðŸ“– Usage

### Transformer Approach (Rotten Tomatoes Analysis)
```python
# Load: "1) Similarity Using Transformer Neural Network.ipynb"
# 1. Load BGE-M3 model
# 2. Process movie review data
# 3. Compute similarity scores
# 4. Compare with GPT results
```

### Siamese Network Approach
```python
# Load: "2) Similarity Using Siamese Neural Network.ipynb"
# 1. Prepare training data
# 2. Train the model
# 3. Evaluate performance
```

## ðŸ“ˆ Results

| Aspect | Transformer | Siamese Network |
|--------|-------------|-----------------|
| **Model** | Pre-trained BGE-M3 | Custom BiLSTM |
| **Training** | None (zero-shot) | 193 sentence pairs |
| **Performance** | High accuracy | ~47% validation accuracy |
| **Speed** | Fast inference | Slower training + inference |

### Similarity Examples:

**Transformer:**
- "I had a bad day" vs "I had so much fun" â†’ 0.638 (Moderate)
- "I had a bad day" vs "Everything was terrible today" â†’ 0.798 (Strong)

**Siamese Network:**
- "I had a bad day" vs "I had so much fun" â†’ 0.492 (Weak)
- "I had a bad day" vs "Everything was terrible today" â†’ 0.500 (Weak)

## ðŸ” Key Findings

### Rotten Tomatoes Analysis:
- Critics and audiences often have similar opinions
- Transformer model effectively captures nuanced differences
- GPT model agrees with cosine similarity classifications
- There's often more common ground than expected

### Approach Comparison:
- **Transformer**: Ready-to-use, high performance, no training needed
- **Siamese Network**: Customizable but requires substantial data and training

## ðŸš€ Future Improvements

### For Rotten Tomatoes:
- Expand dataset with more movies and genres
- Add temporal analysis
- Create interactive dashboard

### For Models:
- Try different embedding models
- Collect larger training datasets
- Optimize hyperparameters

## ðŸ“ Conclusion

This project demonstrates two approaches to sentence similarity, focusing on movie review analysis. The transformer approach proves more practical for immediate use, while the Siamese network shows potential for customization with adequate data.

The analysis revealed that critics and audiences aren't as different as commonly believed, with quantitative NLP techniques showing more common ground than expected. 